import pytest
from urlfinderlib.tokenizer import UTF8Tokenizer

bytes_0_char = b'Test with 0 chars'
expected_0_char = []


bytes_1_char = b'%(char)sTest with 1 char'
expected_1_char = []


bytes_2_char = b'%(char)sTest%(char)s with 1 char'
expected_2_char = ['Test']


bytes_3_char = b'%(char)sTest%(char)s with%(char)s 1 char'
expected_3_char = ['Test', ' with']


def generate_same_character_decorator(character: bytes) -> pytest.mark.parametrize:
    return pytest.mark.parametrize('char_blob,expected', [
        (bytes_0_char, expected_0_char),
        (bytes_1_char % {b'char': character}, expected_1_char),
        (bytes_2_char % {b'char': character}, expected_2_char),
        (bytes_3_char % {b'char': character}, expected_3_char)
    ])


bytes_0_open_close = b'Test with 0 open close chars'
expected_0_open_close = []


bytes_1_open_close = b'%(open)sTest with 1 open close chars'
expected_1_open_close = []


bytes_2_open_close = b'%(open)sTest%(close)s with 2 open close chars'
expected_2_open_close = ['Test']


bytes_3_open_close = b'%(open)sTest%(close)s with 3%(close)s open close chars'
expected_3_open_close = ['Test', 'Test%(close)s with 3']


def generate_open_close_decorator(open_char: bytes, close_char: bytes) -> pytest.mark.parametrize:
    return pytest.mark.parametrize('open_close_blob,expected', [
        (bytes_0_open_close, expected_0_open_close),
        (bytes_1_open_close % {b'open': open_char, b'close': close_char}, expected_1_open_close),
        (bytes_2_open_close % {b'open': open_char, b'close': close_char}, expected_2_open_close),
        (bytes_3_open_close % {b'open': open_char, b'close': close_char},
         [x % {'open': open_char.decode('utf8'), 'close': close_char.decode('utf8')} for x in expected_3_open_close])
    ])


def test_get_ascii_strings():
    tok = UTF8Tokenizer(b'This is\x8aa test with strings and \xc2\xa9 unicode characters')
    expected = ['This is', 'a test with strings and ', ' unicode characters']
    results = tok.get_ascii_strings(length=4)
    assert sorted(results) == sorted(expected)

    expected = ['a test with strings and ', ' unicode characters']
    results = tok.get_ascii_strings(length=8)
    assert sorted(results) == sorted(expected)


def test_get_byte_lines():
    test_bytes = b'''This is a test
that has
multiple
lines.'''
    tok = UTF8Tokenizer(test_bytes)
    expected = ['This is a test', 'that has', 'multiple', 'lines.']
    results = list(tok.get_line_tokens())
    assert sorted(results) == sorted(expected)


def test_get_split_bytes():
    tok = UTF8Tokenizer(b'This is a simple\ttest.')
    expected = ['This', 'is', 'a', 'simple', 'test.']
    results = tok.get_split_tokens()
    assert sorted(results) == sorted(expected)


def test_get_split_bytes_after_replace():
    tok = UTF8Tokenizer(b'This is a "simple"\t<test].')
    expected = ['This', 'is', 'a', 'simple', 'test', '.']
    results = tok.get_split_tokens_after_replace(['"', '<', ']'])
    assert sorted(results) == sorted(expected)


@generate_open_close_decorator(b'<', b'>')
def test_get_tokens_between_angle_brackets(open_close_blob, expected):
    tok = UTF8Tokenizer(open_close_blob)
    results = tok.get_tokens_between_angle_brackets(strict=False)
    assert sorted(results) == sorted(expected)


@generate_same_character_decorator(b'`')
def test_get_tokens_between_backticks(char_blob, expected):
    tok = UTF8Tokenizer(char_blob)
    results = tok.get_tokens_between_backticks()
    assert sorted(results) == sorted(expected)


@generate_open_close_decorator(b'[', b']')
def test_get_tokens_between_brackets(open_close_blob, expected):
    tok = UTF8Tokenizer(open_close_blob)
    results = tok.get_tokens_between_brackets(strict=False)
    assert sorted(results) == sorted(expected)


@generate_open_close_decorator(b'{', b'}')
def test_get_tokens_between_curly_brackets(open_close_blob, expected):
    tok = UTF8Tokenizer(open_close_blob)
    results = tok.get_tokens_between_curly_brackets(strict=False)
    assert sorted(results) == sorted(expected)


@generate_same_character_decorator(b'"')
def test_get_tokens_between_double_quotes(char_blob, expected):
    tok = UTF8Tokenizer(char_blob)
    results = tok.get_tokens_between_double_quotes()
    assert sorted(results) == sorted(expected)


def test_get_tokens_between_open_and_close_sequence():
    tok = UTF8Tokenizer(b'(https://www.domain.com)')
    expected = ['https://www.domain.com']
    results = tok.get_tokens_between_open_and_close_sequence('(http', ')')
    assert sorted(results) == sorted(expected)


def test_get_tokens_between_open_and_close_sequence_strict():
    blob = b'''<</S/URI/URI(http://domain.com/URI)>>
<</Type/Action/S/URI/URI(http://domain2.com) >>
<< /A << /S /URI /Type /Action /URI (http://domain3.com) >>
'''

    expected = [
        'http://domain.com/URI',
        'http://domain2.com',
        'http://domain3.com'
    ]

    tok = UTF8Tokenizer(blob)
    results = list(tok.get_tokens_between_open_and_close_sequence('(', ')', strict=True))
    assert sorted(results) == sorted(expected)


@generate_open_close_decorator(b'(', b')')
def test_get_tokens_between_parentheses(open_close_blob, expected):
    tok = UTF8Tokenizer(open_close_blob)
    results = tok.get_tokens_between_parentheses(strict=False)
    assert sorted(results) == sorted(expected)


@generate_same_character_decorator(b"'")
def test_get_tokens_between_single_quotes(char_blob, expected):
    tok = UTF8Tokenizer(char_blob)
    results = tok.get_tokens_between_single_quotes()
    assert sorted(results) == sorted(expected)


def test_get_tokens_between_spaces():
    tok = UTF8Tokenizer(b'This is a simple test with spaces in it.')
    expected = ['is', 'a', 'simple', 'test', 'with', 'spaces', 'in']
    results = tok.get_tokens_between_spaces()
    assert sorted(results) == sorted(expected)


def test_get_tokens_between_spaces_after_replace():
    tok = UTF8Tokenizer(b'This is a "simple" <test].')
    expected = ['is', 'a', 'simple', 'test']
    results = tok.get_tokens_between_spaces_after_replace(['"', '<', ']'])
    assert sorted(results) == sorted(expected)


def test_non_utf8():
    tok = UTF8Tokenizer(b'This is a simple\x00 test\x8a.')
    expected = ['This', 'is', 'a', 'simple\x00', 'test.']
    results = tok.get_split_tokens()
    assert sorted(results) == sorted(expected)
